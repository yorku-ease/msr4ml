Defines the algorithm or method (optimizer) to use for model training. The selected algorithm or method defines how the model should change the attributes of the neural network, such as weights and learning rate. Optimizers solve optimization problems and make more accurate updates to attributes to reduce learning losses.


Options:

- **Adadelta**
    -  To learn about Adadelta, see <a href="https://arxiv.org/abs/1212.5701" target="_blank" >ADADELTA: An Adaptive Learning Rate Method</a>. 
- **Adam**
    - To learn about Adam, see <a href="https://arxiv.org/abs/1412.6980" target="_blank" >Adam: A Method for Stochastic Optimization</a>. 
- **AdamW**
    - To learn about AdamW, see <a href="https://arxiv.org/abs/1711.05101" target="_blank" >Decoupled Weight Decay Regularization</a>.
- **AdamW8bit**
    - To learn about AdamW, see <a href="https://arxiv.org/abs/1711.05101" target="_blank" >Decoupled Weight Decay Regularization</a>.
- **RMSprop** 
    - To learn about RMSprop, see <a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" target="_blank" >Neural Networks for Machine Learning</a>.
- **SGD** 
    - H2O LLM Studio uses a stochastic gradient descent optimizer.